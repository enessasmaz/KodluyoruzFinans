{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from jpype import JClass, JString, getDefaultJVMPath, shutdownJVM, startJVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZEMBEREK_PATH: str = join('bin', 'zemberek-full.jar') # Zemberek jar dosyasının nerede olduğu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "startJVM(\n",
    "        getDefaultJVMPath(),\n",
    "        '-ea',\n",
    "        f'-Djava.class.path={ZEMBEREK_PATH}',\n",
    "        convertStrings=False\n",
    "    ) # arkaplanda çalışacak java'yı açıyorsun burada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TurkishMorphology: JClass = JClass('zemberek.morphology.TurkishMorphology') # burada java'da import etmişsin gibi düşün\n",
    "WordAnalysis: JClass = JClass('zemberek.morphology.analysis.WordAnalysis') # aynı şekilde\n",
    "\n",
    "morphology: TurkishMorphology = TurkishMorphology.createWithDefaults() # burada da o import'u kullanıyorsun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-16d8a39852f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mresults\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mWordAnalysis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmorphology\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mJString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m+=\u001b[0m  \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetStems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "text = \"denemelerdekilerden kalemlik ve tıpçı yatırmama başarıszlıklar\"\n",
    "a = \"\"\n",
    "for t in text.split():\n",
    "    results: WordAnalysis = morphology.analyze(JString(t))\n",
    "    a +=  str(list(results)[0].getStems()[0]) + \" \" \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results: WordAnalysis = morphology.analyze(JString(\"herşey\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[denemek:Verb] dene:Verb+me:Neg|me:Inf2→Noun+ler:A3pl+imiz:P1pl+de:Loc|ki:Rel→Adj|Zero→Noun+ler:A3pl+de:Loc\n",
      "[demek:Verb] de:Verb|n:Pass→Verb+eme:Unable|me:Inf2→Noun+ler:A3pl+imiz:P1pl+de:Loc|ki:Rel→Adj|Zero→Noun+ler:A3pl+de:Loc\n"
     ]
    }
   ],
   "source": [
    "for r in results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SingleAnalysis' object has no attribute 'getStems'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-78cf60118e79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetStems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'SingleAnalysis' object has no attribute 'getStems'"
     ]
    }
   ],
   "source": [
    "list(results)[0].getStems()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[herşey:Noun] herşey:Noun+A3sg\n",
      "\tStems = herşey\n",
      "\tLemmas = herşey\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for result in results:\n",
    "        print(\n",
    "            f'{str(result.formatLong())}'\n",
    "            f'\\n\\tStems ='\n",
    "            f' {\", \".join([str(result) for result in result.getStems()])}'\n",
    "            f'\\n\\tLemmas ='\n",
    "            f' {\", \".join([str(result) for result in result.getLemmas()])}'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uyumamak\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if \"Neg\" in str(list(results)[0].formatLong()):\n",
    "    print(list(results)[0].getLemmas()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stem(results):\n",
    "    return str(results).split(', ')[2].split(\" \")[0].split(':')[0].split('[')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'zemberek.morphology.analysis.WordAnalysis' object has no attribute 'getStems'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-bb614d565bcb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetStems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'zemberek.morphology.analysis.WordAnalysis' object has no attribute 'getStems'"
     ]
    }
   ],
   "source": [
    "results.getStems()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"denemelerdekilerden kalemlik ve tıpçı\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['denemelerdekilerden', 'kalemlik', 've', 'tıpçı']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting zemberek-python\n",
      "  Downloading zemberek_python-0.1.0-py3-none-any.whl (93.6 MB)\n",
      "Collecting numpy>=1.18.2\n",
      "  Downloading numpy-1.19.2-cp37-cp37m-win_amd64.whl (12.9 MB)\n",
      "Collecting antlr4-python3-runtime>=4.8\n",
      "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
      "Building wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): started\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141234 sha256=469cab69dc7c10e2ea32450fc2420c4efe8818506d74ce6dfee7e58c5ea856a7\n",
      "  Stored in directory: c:\\users\\emirhan\\appdata\\local\\pip\\cache\\wheels\\ca\\33\\b7\\336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: numpy, antlr4-python3-runtime, zemberek-python\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.1\n",
      "    Uninstalling numpy-1.18.1:\n",
      "      Successfully uninstalled numpy-1.18.1\n",
      "Successfully installed antlr4-python3-runtime-4.8 numpy-1.19.2 zemberek-python-0.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install zemberek-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "\n",
    "from zemberek import (\n",
    "    TurkishSpellChecker,\n",
    "    TurkishSentenceNormalizer,\n",
    "    TurkishSentenceExtractor,\n",
    "    TurkishMorphology,\n",
    "    TurkishTokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-10 14:22:33,174 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 12.438500165939331\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "examples = [\"Yrn okua gidicem\",\n",
    "            \"Tmm, yarin havuza giricem ve aksama kadar yaticam :)\",\n",
    "            \"ah aynen ya annemde fark ettı siz evinizden cıkmayın diyo\",\n",
    "            \"gercek mı bu? Yuh! Artık unutulması bile beklenmiyo\",\n",
    "            \"Hayır hayat telaşm olmasa alacam buraları gökdelen dikicem.\",\n",
    "            \"yok hocam kesınlıkle oyle birşey yok\",\n",
    "            \"herseyi soyle hayatında olmaması gerek bence boyle ınsanların falan baskı yapıyosa\",\n",
    "            \"email adresim zemberek_python@loodos.com\",\n",
    "            \"Kredi başvrusu yapmk istiyrum.\",\n",
    "            \"Bankanizin hesp blgilerini ogrenmek istyorum.\"]\n",
    "\n",
    "morphology = TurkishMorphology.create_with_defaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-10 14:06:42,030 - __main__ - INFO\n",
      "Msg: Normalization instance created in: 31.237332344055176 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "normalizer = TurkishSentenceNormalizer(morphology)\n",
    "logger.info(f\"Normalization instance created in: {time.time() - start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yrn okua gidicem\n",
      "yarın okula gideceğim \n",
      "\n",
      "Tmm, yarin havuza giricem ve aksama kadar yaticam :)\n",
      "tamam , yarın havuza gireceğim ve akşama kadar yatıcam :) \n",
      "\n",
      "ah aynen ya annemde fark ettı siz evinizden cıkmayın diyo\n",
      "ah aynen ya annemde fark etti siz evinizden çıkmayın diyor \n",
      "\n",
      "gercek mı bu? Yuh! Artık unutulması bile beklenmiyo\n",
      "gerçek mi bu ? yuh ! artık uyutulması bile beklenmiyor \n",
      "\n",
      "Hayır hayat telaşm olmasa alacam buraları gökdelen dikicem.\n",
      "hayır hayat telaşı olmasa alacam buraları gökdelen dikeceğim . \n",
      "\n",
      "yok hocam kesınlıkle oyle birşey yok\n",
      "yok hocam kesinlikle öyle bir şey yok \n",
      "\n",
      "herseyi soyle hayatında olmaması gerek bence boyle ınsanların falan baskı yapıyosa\n",
      "herşeyi söyle hayatında olmaması gerek bence böyle insanların falan baskı yapıyorsa \n",
      "\n",
      "email adresim zemberek_python@loodos.com\n",
      "mail adresim zemberek_python@loodos.com \n",
      "\n",
      "Kredi başvrusu yapmk istiyrum.\n",
      "kredi başvurusu yapmak istiyorum . \n",
      "\n",
      "Bankanizin hesp blgilerini ogrenmek istyorum.\n",
      "bankanızın hesap bilgilerini öğrenmek istiyorum . \n",
      "\n",
      "2020-10-10 14:07:57,483 - __main__ - INFO\n",
      "Msg: Sentences normalized in: 4.445354223251343 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for example in examples:\n",
    "    print(example)\n",
    "    print(normalizer.normalize(example), \"\\n\")\n",
    "logger.info(f\"Sentences normalized in: {time.time() - start} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-10 14:09:06,219 - __main__ - INFO\n",
      "Msg: Spell checker instance created in: 24.90722680091858 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "sc = TurkishSpellChecker(morphology)\n",
    "logger.info(f\"Spell checker instance created in: {time.time() - start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okuyablirim = okuyabilirim\n",
      "tartısıyor = tartışıyor tartılıyor\n",
      "Ankar'ada = Ankara'da Ankara'ya Ankara'dan Ankara'ma Ankara'na Ankara'nda Antara'da Ankaray'a Ankara'ca Angara'da Ankara'mda Ankaray'da Anakara'da\n",
      "knlıca = kanlıca kanlıca kalıca kılıca anlıca unlıca kınlıca onlıca\n",
      "yapablrim = \n",
      "kıredi = kredi küredi\n",
      "geldm = geldi geldim gelem gelim\n",
      "geliyom = geliyor geliyim\n",
      "aldm = aldı alım aldım alem alim alim alam aldo aldi Aldi'm Al'da Al'ım Al'dım Al'dı alda Aldo'm\n",
      "asln = asla aslan aslan aslı aslı asli aslen asan asın As'lı assn asin aslın As'lın aslin As'la Aslı'n As'lan As'ın\n",
      "2020-10-10 14:09:14,363 - __main__ - INFO\n",
      "Msg: Spells checked in: 0.446382999420166 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SPELLING SUGGESTION\n",
    "li = [\"okuyablirim\", \"tartısıyor\", \"Ankar'ada\", \"knlıca\", \"yapablrim\", \"kıredi\", \"geldm\", \"geliyom\", \"aldm\", \"asln\"]\n",
    "start = time.time()\n",
    "for word in li:\n",
    "    print(word + \" = \" + ' '.join(sc.suggest_for_word(word)))\n",
    "logger.info(f\"Spells checked in: {time.time() - start} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractor instance created in:  0.016954660415649414  s\n"
     ]
    }
   ],
   "source": [
    "# SENTENCE BOUNDARY DETECTION\n",
    "start = time.time()\n",
    "extractor = TurkishSentenceExtractor()\n",
    "print(\"Extractor instance created in: \", time.time() - start, \" s\")\n",
    "\n",
    "text = \"İnsanoğlu aslında ne para ne sevgi ne kariyer ne şöhret ne de çevre ile sonsuza dek mutlu olabilecek bir \" \\\n",
    "       \"yapıya sahiptir. Dış kaynaklardan gelebilecek bu mutluluklar sadece belirli bir zaman için insanı mutlu \" \\\n",
    "       \"kılıyor. Kişi bu kaynakları elde ettiği zaman belirli bir dönem için kendini iyi hissediyor, ancak alışma \" \\\n",
    "       \"dönemine girdiği andan itibaren bu iyilik hali hızla tükeniyor. Mutlu olma sanatının özü bu değildir. Gerçek \" \\\n",
    "       \"mutluluk, kişinin her türlü olaya ve duruma karşı kendini pozitif tutarak mutlu hissedebilmesi halidir. Bu \" \\\n",
    "       \"davranış şeklini edinen insan, zor günlerde güçlü, mutlu günlerde zevk alan biri olur ve mutluluğu kalıcı \" \\\n",
    "       \"kılar. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences separated in 0.0009987354278564453s\n",
      "İnsanoğlu aslında ne para ne sevgi ne kariyer ne şöhret ne de çevre ile sonsuza dek mutlu olabilecek bir yapıya sahiptir.\n",
      "Dış kaynaklardan gelebilecek bu mutluluklar sadece belirli bir zaman için insanı mutlu kılıyor.\n",
      "Kişi bu kaynakları elde ettiği zaman belirli bir dönem için kendini iyi hissediyor, ancak alışma dönemine girdiği andan itibaren bu iyilik hali hızla tükeniyor.\n",
      "Mutlu olma sanatının özü bu değildir.\n",
      "Gerçek mutluluk, kişinin her türlü olaya ve duruma karşı kendini pozitif tutarak mutlu hissedebilmesi halidir.\n",
      "Bu davranış şeklini edinen insan, zor günlerde güçlü, mutlu günlerde zevk alan biri olur ve mutluluğu kalıcı kılar.\n",
      "\n",
      "\n",
      "[Kale:Noun, Prop] kale:Noun+A3sg+m:P1sg+in:Gen\n",
      "[kale:Noun] kale:Noun+A3sg+m:P1sg+in:Gen\n",
      "[kalem:Noun] kalem:Noun+A3sg+in:Gen\n",
      "[kalem:Noun] kalem:Noun+A3sg+in:P2sg\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "sentences = extractor.from_paragraph(text)\n",
    "print(f\"Sentences separated in {time.time() - start}s\")\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "print(\"\\n\")\n",
    "\n",
    "# SINGLE WORD MORPHOLOGICAL ANALYSIS\n",
    "results = morphology.analyze.cache_\n",
    "for result in results:\n",
    "    print(result)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[uyumak:Verb] uyu:Verb+ma:Neg|mak:Inf1→Noun+A3sg\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SINGLE WORD MORPHOLOGICAL ANALYSIS\n",
    "results = morphology.analyze(\"uyumamak\")\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[uyumak:Verb] uyu:Verb+ma:Neg|mak:Inf1→Noun+A3sg\n"
     ]
    }
   ],
   "source": [
    "print(list(results)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content =  Saat\n",
      "Type =  Word\n",
      "Start =  0\n",
      "Stop =  3 \n",
      "\n",
      "Content =  12:00\n",
      "Type =  Time\n",
      "Start =  5\n",
      "Stop =  9 \n",
      "\n",
      "Content =  .\n",
      "Type =  Punctuation\n",
      "Start =  10\n",
      "Stop =  10 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TOKENIZATION\n",
    "tokenizer = TurkishTokenizer.DEFAULT\n",
    "\n",
    "tokens = tokenizer.tokenize(\"Saat 12:00.\")\n",
    "for token in tokens:\n",
    "    print('Content = ', token.content)\n",
    "    print('Type = ', token.type_.name)\n",
    "    print('Start = ', token.start)\n",
    "    print('Stop = ', token.end, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT CLEANER:\n",
    "    METİN\n",
    "    METNİ TEMİZLİYORUZ (NOKTALAMA İŞARETLERİ,KÜÇÜLTMEK)\n",
    "    StopWords ? \n",
    "    ZEMBEREK\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
